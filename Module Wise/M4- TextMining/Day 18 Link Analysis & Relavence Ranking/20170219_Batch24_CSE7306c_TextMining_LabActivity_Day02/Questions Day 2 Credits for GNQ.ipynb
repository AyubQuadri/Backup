{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Questions:\n",
    "•How many words are there in the text? How many sentences are there in the text?\n",
    "•How many unique words are there in the text?\n",
    "•What is the average length of a word in the text?\n",
    "Optional: (But submitting the solution on piazza will give you additional credit in GNQ)\n",
    "\n",
    "•Remove the stop words and store the tokens in a list.\n",
    "•What are the 10 most common unigrams in the processed text?\n",
    "•What are the 10 most common bigrams in the processed text?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\quadris\\Desktop\\My Learning\\INSOFE CPEE\\Day 18\\20170219_Batch24_CSE7306c_TextMining_LabActivity_Day02\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.chdir('C:/Users/quadris/Desktop/My Learning/INSOFE CPEE/Day 18/20170219_Batch24_CSE7306c_TextMining_LabActivity_Day02')\n",
    "print(os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['.ipynb_checkpoints', '20170219_Batch24_CSE7306_Day02_LabActivity_PageRank.ipynb', '20170219_Batch24_CSE7306_Day02_LabActivity_TFIDF.ipynb', '20170219_Batch24_CSE7306_Day02_Lab_Preprocessing.ipynb', 'ende.json', 'english_german_articles.txt', 'Questions Day 2 Credits for GNQ.ipynb', 'shakespeare-macbeth.txt', 'sherlock.txt', 'TfIdfManual.xlsx', 'word_freqs.json']\n"
     ]
    }
   ],
   "source": [
    "print(os.listdir(os.getcwd()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('sherlock.txt','r') as file_:\n",
    "    string = file_.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['At', 'Waterloo', 'we', 'were', 'fortunate', 'in', 'catching', 'a', 'do', \"n't\", 'train', 'for', 'Leatherhead', ',', 'where', 'we', 'hired', 'a', 'trap', 'at', 'the', 'station', 'inn', 'and', 'drove', 'for', 'four', 'or', 'five', 'miles', 'through', 'the', 'lovely', 'Surrey', 'lanes', '.', 'It', 'was', 'a', 'perfect', 'day', ',', 'with', 'a', 'bright', 'sun', 'and', 'a', 'few', 'fleecy', 'clouds', 'in', 'the', 'heavens', '.', 'The', 'trees', 'and', 'wayside', 'hedges', 'were', 'just', 'throwing', 'out', 'their', 'first', 'green', 'shoots', ',', 'and', 'the', 'air', 'was', 'full', 'of', 'the', 'pleasant', 'smell', 'of', 'the', 'moist', 'earth', '.', 'To', 'me', 'at', 'least', 'there', 'was', 'a', 'strange', 'contrast', 'between', 'the', 'sweet', 'promise', 'of', 'the', 'spring', 'and', 'this', 'sinister', 'quest', 'upon', 'which', 'we', 'were', 'engaged', '.', 'My', 'companion', 'sat', 'in', 'the', 'front', 'of', 'the', 'trap', ',', 'his', 'arms', 'folded', ',', 'his', 'hat', 'pulled', 'down', 'over', 'his', 'eyes', ',', 'and', 'his', 'chin', 'sunk', 'upon', 'his', 'breast', ',', 'buried', 'in', 'the', 'deepest', 'thought', '.', 'Suddenly', ',', 'however', ',', 'he', 'started', ',', 'tapped', 'me', 'on', 'the', 'shoulder', ',', 'and', 'pointed', 'over', 'the', 'meadows', '.']\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "# Tockenizing the words\n",
    "tokens = nltk.tokenize.word_tokenize(string)\n",
    "print(tokens)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q1. How many words are there in the text? How many sentences are there in the text?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word count frequency before any pre processing <FreqDist with 104 samples and 164 outcomes> \n",
      "Number of sentences in  \"sterlock.txt\" are 6\n"
     ]
    }
   ],
   "source": [
    "word_count = nltk.FreqDist() #WE INITIALIZE AN EMPTY FREQUENCY COUNTER\n",
    "\n",
    "for token in tokens:\n",
    "    word_count[token] += 1\n",
    "\n",
    "print('Word count frequency before any pre processing {} '.format(word_count))\n",
    "print('Number of sentences in  \"sterlock.txt\" are {}'.format(word_count[token]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stop Words Removal\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['At', 'Waterloo', 'fortunate', 'catching', \"n't\", 'train', 'Leatherhead', ',', 'hired', 'trap', 'station', 'inn', 'drove', 'four', 'five', 'miles', 'lovely', 'Surrey', 'lanes', '.', 'It', 'perfect', 'day', ',', 'bright', 'sun', 'fleecy', 'clouds', 'heavens', '.', 'The', 'trees', 'wayside', 'hedges', 'throwing', 'first', 'green', 'shoots', ',', 'air', 'full', 'pleasant', 'smell', 'moist', 'earth', '.', 'To', 'least', 'strange', 'contrast', 'sweet', 'promise', 'spring', 'sinister', 'quest', 'upon', 'engaged', '.', 'My', 'companion', 'sat', 'front', 'trap', ',', 'arms', 'folded', ',', 'hat', 'pulled', 'eyes', ',', 'chin', 'sunk', 'upon', 'breast', ',', 'buried', 'deepest', 'thought', '.', 'Suddenly', ',', 'however', ',', 'started', ',', 'tapped', 'shoulder', ',', 'pointed', 'meadows', '.']\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "# Reading the stopwords from the corpus\n",
    "stop = stopwords.words('english')\n",
    "# Removing the stop words from the text\n",
    "AfterStopWord_tokens = [token for token in tokens if token not in stop]\n",
    "\n",
    "print(AfterStopWord_tokens[:100])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q1. Remove the stop words and store the tokens in a list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word count frequency before any pre processing <FreqDist with 75 samples and 92 outcomes> \n",
      "Number of sentences in  \"sterlock.txt\" are 6\n"
     ]
    }
   ],
   "source": [
    "word_count = nltk.FreqDist() #WE INITIALIZE AN EMPTY FREQUENCY COUNTER\n",
    "\n",
    "for token in AfterStopWord_tokens:\n",
    "    word_count[token] += 1\n",
    "\n",
    "print('Word count frequency before any pre processing {} '.format(word_count))\n",
    "print('Number of sentences in  \"sterlock.txt\" are {}'.format(word_count[token]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "processing the string "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['At', 'Waterloo', 'fortunate', 'catching', \"n't\", 'train', 'Leatherhead', ',', 'hired', 'trap', 'station', 'inn', 'drove', 'four', 'five', 'mile', 'lovely', 'Surrey', 'lane', '.', 'It', 'perfect', 'day', ',', 'bright', 'sun', 'fleecy', 'cloud', 'heaven', '.', 'The', 'tree', 'wayside', 'hedge', 'throwing', 'first', 'green', 'shoot', ',', 'air', 'full', 'pleasant', 'smell', 'moist', 'earth', '.', 'To', 'least', 'strange', 'contrast', 'sweet', 'promise', 'spring', 'sinister', 'quest', 'upon', 'engaged', '.', 'My', 'companion', 'sat', 'front', 'trap', ',', 'arm', 'folded', ',', 'hat', 'pulled', 'eye', ',', 'chin', 'sunk', 'upon', 'breast', ',', 'buried', 'deepest', 'thought', '.', 'Suddenly', ',', 'however', ',', 'started', ',', 'tapped', 'shoulder', ',', 'pointed', 'meadow', '.']\n"
     ]
    }
   ],
   "source": [
    "lmtzr = nltk.stem.WordNetLemmatizer()\n",
    "tokens = [lmtzr.lemmatize(token) for token in AfterStopWord_tokens]\n",
    "print(tokens[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Applying the necessary text preprocessing steps on the data\n",
    "def process_text(text):\n",
    "    sentences = nltk.tokenize.sent_tokenize(text)\n",
    "    sentence_tokens = [nltk.tokenize.word_tokenize(sentence) for sentence in sentences]\n",
    "    tokens = []\n",
    "    for sentence in sentence_tokens:\n",
    "        sent = []\n",
    "        for word in sentence:\n",
    "            if word.lower() not in stop:\n",
    "                sent.append(word.lower())\n",
    "        tokens.append(sent)\n",
    "    ##THE SAME FOR LOOP CAN BE WRITTEN AS FOLLOWS\n",
    "    ##tokens = [[word.lower() for word in sent if word not in stop] for sent in sentence_tokens]\n",
    "    tokens = [[lmtzr.lemmatize(word) for word in sent] for sent in tokens]\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "At Waterloo we were fortunate in catching a don't train for Leatherhead, where we hired a trap at the station inn and drove for four or five miles through the lovely Surrey lanes. \n",
      "It was a perfect day, with a bright sun and a few fleecy clouds in the heavens. \n",
      "The trees and wayside hedges were just throwing out their first green shoots, and the air was full of the pleasant smell of the moist earth. To me at least there was a strange contrast between the sweet promise of the spring and this sinister quest upon which we were engaged. \n",
      "My companion sat in the front of the trap, his arms folded, his hat pulled down over his eyes, and his chin sunk upon his breast, buried in the deepest thought. \n",
      "Suddenly, however, he started, tapped me on the shoulder, and pointed over the meadows.\n",
      "\n",
      "[['waterloo', 'fortunate', 'catching', \"n't\", 'train', 'leatherhead', ',', 'hired', 'trap', 'station', 'inn', 'drove', 'four', 'five', 'mile', 'lovely', 'surrey', 'lane', '.'], ['perfect', 'day', ',', 'bright', 'sun', 'fleecy', 'cloud', 'heaven', '.'], ['tree', 'wayside', 'hedge', 'throwing', 'first', 'green', 'shoot', ',', 'air', 'full', 'pleasant', 'smell', 'moist', 'earth', '.'], ['least', 'strange', 'contrast', 'sweet', 'promise', 'spring', 'sinister', 'quest', 'upon', 'engaged', '.'], ['companion', 'sat', 'front', 'trap', ',', 'arm', 'folded', ',', 'hat', 'pulled', 'eye', ',', 'chin', 'sunk', 'upon', 'breast', ',', 'buried', 'deepest', 'thought', '.'], ['suddenly', ',', 'however', ',', 'started', ',', 'tapped', 'shoulder', ',', 'pointed', 'meadow', '.']]\n"
     ]
    }
   ],
   "source": [
    "# Calling the function\n",
    "sentenced_tokens = process_text(string)\n",
    "print(string)\n",
    "print(sentenced_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unigram model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('At',)\n",
      "('Waterloo',)\n",
      "('fortunate',)\n",
      "('catching',)\n",
      "(\"n't\",)\n",
      "('train',)\n",
      "('Leatherhead',)\n",
      "(',',)\n",
      "('hired',)\n",
      "('trap',)\n",
      "('station',)\n",
      "('inn',)\n",
      "('drove',)\n",
      "('four',)\n",
      "('five',)\n",
      "('mile',)\n",
      "('lovely',)\n",
      "('Surrey',)\n",
      "('lane',)\n",
      "('.',)\n",
      "('It',)\n",
      "('perfect',)\n",
      "('day',)\n",
      "(',',)\n",
      "('bright',)\n",
      "('sun',)\n",
      "('fleecy',)\n",
      "('cloud',)\n",
      "('heaven',)\n",
      "('.',)\n",
      "('The',)\n",
      "('tree',)\n",
      "('wayside',)\n",
      "('hedge',)\n",
      "('throwing',)\n",
      "('first',)\n",
      "('green',)\n",
      "('shoot',)\n",
      "(',',)\n",
      "('air',)\n",
      "('full',)\n",
      "('pleasant',)\n",
      "('smell',)\n",
      "('moist',)\n",
      "('earth',)\n",
      "('.',)\n",
      "('To',)\n",
      "('least',)\n",
      "('strange',)\n",
      "('contrast',)\n",
      "('sweet',)\n",
      "('promise',)\n",
      "('spring',)\n",
      "('sinister',)\n",
      "('quest',)\n",
      "('upon',)\n",
      "('engaged',)\n",
      "('.',)\n",
      "('My',)\n",
      "('companion',)\n",
      "('sat',)\n",
      "('front',)\n",
      "('trap',)\n",
      "(',',)\n",
      "('arm',)\n",
      "('folded',)\n",
      "(',',)\n",
      "('hat',)\n",
      "('pulled',)\n",
      "('eye',)\n",
      "(',',)\n",
      "('chin',)\n",
      "('sunk',)\n",
      "('upon',)\n",
      "('breast',)\n",
      "(',',)\n",
      "('buried',)\n",
      "('deepest',)\n",
      "('thought',)\n",
      "('.',)\n",
      "('Suddenly',)\n",
      "(',',)\n",
      "('however',)\n",
      "(',',)\n",
      "('started',)\n",
      "(',',)\n",
      "('tapped',)\n",
      "('shoulder',)\n",
      "(',',)\n",
      "('pointed',)\n",
      "('meadow',)\n",
      "('.',)\n"
     ]
    }
   ],
   "source": [
    "bigrams = list(nltk.ngrams(tokens, 1, pad_right = True, right_pad_symbol='</s>', pad_left=True, left_pad_symbol='<s>'))\n",
    "for ngram in bigrams:\n",
    "    \n",
    "    print(ngram)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
